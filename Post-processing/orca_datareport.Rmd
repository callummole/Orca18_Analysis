---
title: 'Silent Failures in Automation. Pilot dataset Report'
author: "Callum Mole"
output:
  html_document:
    df_print: paged
  html_notebook:
    fig_caption: yes
  pdf_document:
    fig_caption: yes
  word_document:
    fig_caption: yes
---

## Introduction

This document serves as a report for a first run of the pre-registered experiment https://osf.io/s6vam/. The experiment examines silent failures in a highly controlled setting. Each trial starts with automated steering. In some trials a bias is introduced that causes the vehicle to veer off the road either suddenly (within 1.5 s) or gradually (~ 4 s). Participants are required to keep within the road edges and intervene if they feel that it is necessary. They complete the task without distraction or with a easy or difficult distraction. There are also two bend radii used: sharp (40 m) or gradual (80 m).

```{r Load preliminaries, include=FALSE, warning=FALSE}

library("tidyverse")
library(magrittr) #for extra pipe functions
library(cowplot)
library("wesanderson")

#theme for plots on TRANSITION grant.
theme_transition <- theme_classic() +
  theme(strip.background = element_rect(fill=NA,color=NA), 
        strip.text = element_text(face="bold",colour="black",size="8"), 
        axis.title = element_text(face="bold",colour="black",size="8"),
        axis.text.x = element_text(vjust=-.5),
        axis.text.y = element_text(vjust=.5),
        axis.text = element_text(face="plain",colour="black",size="7"),
        legend.text = element_text(face="plain",colour="black",size="7"),
        legend.title = element_text(face="bold",colour="black",size="8"),
        legend.key = element_blank(),
        panel.grid.major.y = element_line(color="grey85",size=.2, linetype = 2))

```


```{r Load data, echo=FALSE, message=FALSE, warning=FALSE}

#set working directory to folder that hosts the binary files.
setwd("C:/Users/psccmo/Orca18_Analysis/Post-Processing/")

#load steergaze data
steergazedata <- readRDS("orca_steergazedata.rds")

#add left bend
steergazedata <- steergazedata %>% 
  mutate(bend = ifelse(trialtype_signed < 0, "left", "right"))

steergazedata[steergazedata$bend == "left", "hangle"] <- steergazedata[steergazedata$bend == "left", "hangle"]*-1 



steergazedata <- steergazedata %>% 
  rename(SWV = SWA) %>% 
  mutate(SWA = SWV * 90)

```


### Technical Errors

This experiment stands as a lesson for the need to conduct a full technical pilot - running a participant through the entire experiment then putting that data through the entire analysis workflow. Due to time pressures we often do _some_ piloting to ostensibly check data saving, condition indexing etc. But we do not very often take the time to process the data through analysis scripts. In this case the data looked like it was saving correctly when piloting. But there were two key errors. First, the distraction performance files were overwritten when the driver was also steering (we still have the baseline - no steering - distraction performance files). Secondly, the "unique" filename for saving individual trials did not include the yawrate offsets in the title. So, in effect these trials were overwritten and only the last six (randomised) trials were saved for each radii, irrespective of yawrates offsets.

This means that we only have a small amount of data to work with, and the amount of trials performed (or, to be more precise, saved) for each condition is random between conditions. Fig 1 shows how many trials we have. The expected trialcount for each condition is 180 trials in the dataset.



```{r data logging, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10.5,fig.height=4.5,fig.cap="Fig 1. Amount of trials in each condition"}

steergaze_trial <- steergazedata  %>% 
  group_by(ppid, radius, yawrate_offset, cogload, block, count, .drop = F) %>% 
  summarize(trialcode = first(trialcode))


steergaze_expanded_counts <- steergaze_trial %>% 
  ungroup() %>% 
  complete(ppid, radius, yawrate_offset, cogload, 
           fill = list(trialcode = 99)) %>% 
  group_by(radius, yawrate_offset, cogload, .drop = FALSE) %>% 
  tally()
           
expected_trialcount <- 30 * 6 #30 participants x 6 trials in each condition.

#change the yawrate_offsets to factors.
steergaze_expanded_counts <- steergaze_expanded_counts %>% 
  mutate(failure_type = case_when(yawrate_offset %in% c(-.2, .15) ~ "None",
                                  yawrate_offset == -9 ~ "Sudden",
                                  yawrate_offset == -1.5 ~ "Gradual"))

steergaze_expanded_counts$cogload <- factor(steergaze_expanded_counts$cogload, levels = c("None", "Easy", "Hard"))
steergaze_expanded_counts$failure_type <- factor(steergaze_expanded_counts$failure_type, levels = c("None", "Gradual", "Sudden"))

#plot counts.
ggplot(steergaze_expanded_counts, aes(y = n, x = factor(cogload), fill = factor(failure_type))) +
  facet_wrap(~radius) +
  theme_transition +
   geom_bar(stat="identity", position=position_dodge()) +
  scale_fill_manual(values = wes_palette("BottleRocket2"), name = "Failure Type") +
  xlab("Cog Load") +
  ylab("Amount of Trials") +
  scale_y_continuous(sec.axis = sec_axis(~./180 * 100, name = "Proportion of Expected Data [%]"))

```

Fig 1 shows that for the distraction conditions we only have 20-30 % of the expected data. For driving without distraction we have slightly more because there are two blocks. For the trials we do have we have complete steering and gaze data, so at the very least the reduced data set will be useful at tweaking the design for an improved re-run, and for developing the modelling architecture.

### Cognitive Load difficulty.

 - baseline differences in reaction time and % correct. 

```{r load cognitive task data, echo=FALSE, message=FALSE, warning=FALSE}

file_path <- "D:/ORCA_DATA/Data" #filepath (~ means to look in user's working directory)
file_lists <- list(list.files(file_path, pattern = "Orca18_Distractor_1_(p|P)"),
                   list.files(file_path, pattern = "Orca18_Distractor_2_(p|P)")) #separate into two blocks so I can loop through.
EoT <- "EndofTrial" #text at end of EndofTrial files. These are the recorded counts at the end of the trial.
WiT <- "WithinTrial" #text at end of WithinTrial files. These are the RT responses within a trial. 

block = 0

#assign dataframes

EoT_dataframe <- data.frame(X=integer(),
                 ppid=character(), 
                 targetoccurence=double(), 
                 targetnumber=integer(),
                 trialn = integer(),
                 EoTScore1 = double(),
                 TargetCount1 = double(),
                 EoTScore2 = double(),
                 TargetCount2 = double(),
                 EoTScore3 = double(),
                 TargetCount3 = double(),
                 block = integer())

WiT_dataframe <- data.frame(X=integer(),
                 ppid=character(), 
                 targetoccurence=double(), 
                 targetnumber=integer(),
                 trialn = integer(),
                 CurrentAudio = character(),
                 RT = double(),
                 ResponseCategory = integer(),
                 Target1 = character(),
                 Target2 = character(),
                 Target3 = character(),
                 block = integer())



for (file_block in file_lists){ #loop through  each block so you can add a block number and add 6 to trial number.
  
  #we want to add 6 to the second block trialn.
  block = block + 1
  if (block == 1){
    trial_add = 0
  } else {
    trial_add = 6
  }
  
  for (file_name in file_block){
    
    print(file_name)
    
    #separate dataframe if EoT or WiT
    
    if (grepl(EoT, file_name)){
    
      EoT_newdata <- read.csv(paste(file_path,'/',file_name, sep = "")) #if already exists add to data frame.
      
      head(EoT_newdata)
      
      EoT_newdata <- EoT_newdata %>% 
        mutate(trialn = trialn + trial_add,
               block = block)
      
      EoT_dataframe <- dplyr::union(EoT_newdata, EoT_dataframe) #add to existing datframe  
        
            
    } else if  (grepl(WiT, file_name)) {
      WiT_newdata <- read.csv(paste(file_path,'/',file_name, sep = "")) #load WithinTrial data
      
      WiT_newdata <- WiT_newdata %>% 
        mutate(trialn = trialn + trial_add,
               block = block)
      
      WiT_dataframe <- dplyr::union(WiT_newdata, WiT_dataframe) #add to existing datframe  
        
    }
  }
}

#head(EoT_dataframe) #view start of dataframe.
#head(WiT_dataframe) #view start of dataframe.

```


Now we have loaded the cognitive load data, let's crunch the numbers.

```{r calculate measures, echo=FALSE, message=FALSE, warning=FALSE}
  
##### WITHIN TRIAL MEASURES ########


WiT_RTfiltered <- filter(WiT_dataframe, RT == -1 | RT >.1) # Returns dataframe for rows where RT was >.1 or -1 (no response) 

WiT_TruePos <- filter(WiT_RTfiltered, ResponseCategory == 1) #create new dataframe only including true positives
  
SummaryRTs <- WiT_TruePos %>% group_by(ppid, trialn) %>% summarise(
  targetnumber = first(targetnumber),
  targetoccurence = first(targetoccurence),
  meanRT = mean(RT),
  stdRT = sd(RT))

#Calculate the amount of each type of responses
SummaryCounts <- WiT_RTfiltered %>% group_by(ppid, trialn) %>% summarise(
  targetnumber = first(targetnumber),
  targetoccurence = first(targetoccurence),
  TruePos = sum(ResponseCategory==1),
  FalseNeg = sum(ResponseCategory==2), 
  FalsePos = sum(ResponseCategory==3),
  TrueNeg = sum(ResponseCategory==4), 
  TotalResponses=n())

SummaryCounts <- mutate(SummaryCounts, Perc_Correct = (TruePos + TrueNeg)/ TotalResponses)


####### END OF TRIAL MEASURES ########
  
#First, replace NA with Zeros for the following code to work. This means I can use the same code on all trials, even though some may have different amounts of targets.
EoT_dataframe[is.na(EoT_dataframe)] <- 0

#Calculate the error for each target.
EoT_dataframe <- mutate(EoT_dataframe, 
                        Error1 = EoTScore1 - TargetCount1,
                        Error2 = EoTScore2 - TargetCount2,
                        Error3 = EoTScore3 - TargetCount3)

#Calculate the total absolute error and divide by targetnumber
EoT_dataframe <- EoT_dataframe %>% 
  mutate(totalcounterror = abs(Error1) + abs(Error2) + abs(Error3),
         totaltargets = abs(TargetCount1) + abs(TargetCount2) + abs(TargetCount3),
         avgcounterror = totalcounterror / targetnumber,
         proportionalcounterror = totalcounterror / totaltargets
         )
                            




########### MERGE DATAFRAMES ############

#merges dataframes for trial measures
SummaryTrialMeasures <- merge(SummaryCounts, SummaryRTs, by = c("ppid","trialn"), all.x = TRUE)

#only select the columns we are interested in 
EoT_avgerror <- select(EoT_dataframe, ppid, trialn, targetnumber, targetoccurence, totalcounterror, totaltargets, avgcounterror, proportionalcounterror)

#merge within trial and EoT measures together
SummaryMeasures <- merge(SummaryTrialMeasures, EoT_avgerror, by = c("ppid","trialn"))

#drop some extra columns created by merging for some unimportant reasons
SummaryMeasures <- select(SummaryMeasures, -targetoccurence.x, -targetnumber.x, -targetoccurence.y, -targetnumber.y)

head(SummaryMeasures)
```

We only have the cognitive performance for baseline (without steering). Here we plot three indices of performance: _Percentage Correct_ (PC, True positives + True negatives / total letters heard); _Reaction Time_ (RT, reaction time for True positives); _Proportional Absolute Count Error_ (average distance from the true count, expressed as a proportion of the total amount of targets heard).

```{r plot Cognitive Task Performance, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10.5,fig.height=4.5,fig.cap="Fig 2. Cognitive Task Performance. A) Percentage Correct. B) Reaction Time. C-D) Absolute Count Error"}

# box plots for absolute count error.

#so I do not repeat myself below
addscale <- scale_fill_manual(values = wes_palette("Cavalcanti1"), name = "Cognitive Load", labels = c("1"="Easy", "3"="Hard"))
changexlabels <- scale_x_discrete(name = "Cognitive Load", labels=c("1" = "Easy", "3" = "Hard"))

#Example 2: plot ACE for each targetnumber
p_rt <- ggplot(data=SummaryMeasures, aes(y=meanRT, x=factor(targetnumber), fill = factor(targetnumber))) + 
  geom_boxplot(outlier.size = 1) +
  addscale +
  theme_transition +
  changexlabels + ylab ("Mean RT for True Positives (s)")
  
p_pc <- ggplot(data=SummaryMeasures, aes(y=Perc_Correct, x=factor(targetnumber), fill = factor(targetnumber))) + 
  geom_boxplot(outlier.size = 1) +
  addscale +
  theme_transition +
  changexlabels + ylab ("Percentage of Targets Responded Correctly (%)")

p_ace <- ggplot(data=SummaryMeasures, aes(y=proportionalcounterror, x=factor(targetnumber), fill = factor(targetnumber))) + 
  geom_boxplot(outlier.size = 1) +
  addscale +
  theme_transition +
  changexlabels + ylab ("Average proportional error in estimated count")

legend <- get_legend(p_rt)
p <- plot_grid(p_rt + theme(legend.position="none"), 
               p_pc + theme(legend.position="none"), 
               p_ace + theme(legend.position="none"),
               labels = c("A", "B","C"), nrow=1)
p_legend <- plot_grid(p, legend, rel_widths = c(4,.5))
show(p_legend)

```
 
Fig 2 shows that people respond differently to each Cognitive Load condition. Participants react slower to heard target letters if they are listening for three targets as compared to one target (Fig 2A). Participants rarely make mistakes (i.e. responding to a distractor letter or not responding to a target letter) in the Easy Cognitive Load condition, they make more mistakes in the hard Cognitive Load condition (Fig 2B). 
 
The interpretation of the last measure, the count error (Fig 2C), is more nuanced. Assessing accuracy of estimated counts is tricky because the amount of occurences of each target in the _Hard_ condition is generally a third of the count of the target in the _Easy_ condition. It is worth noting that the total error (cumulative across multiple targets) is pretty identical across both conditions. How you use then use this total count error to take into account people are responding to more targets _Hard_ than _Easy_ is the debateable bit. For example, estimating five instead of six has the same total error as estimating zero, two, three, for target counts of one, two and three. Yet one could argue that the two responses are qualitatively different because in the second case the participant has entirely missed a target but got the other two spot on. To reflect this I've chosen to express this measure as a proportional of the total amount of targets heard (Fig 2C). In any case, people are only marginally less accurate on this measure in the _Hard_ cognitive load than for _Easy_. It seems that the more precise measures of RT and PC are better indicators of differences in performance.

Therefore, a quick eyeball of the baseline cognitive task performance suggests that people reacted slower and experienced greater difficulty in deciding the appropriate response when there were three targets as compared to when there was only one.

Let's see how this gets reflected in steering measures.

### Steering.

### Gaze

### Steering and Gaze

### with/without automation.

### plots of steering trajectories.

### plots of gaze. 


